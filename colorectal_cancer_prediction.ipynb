{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "Data = pd.read_csv('colorectal_cancer_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for NULL values in the dataset\n",
    "null_values = Data.isnull().sum()\n",
    "\n",
    "# Print columns with NULL values\n",
    "print(\"Columns with NULL values:\")\n",
    "print(null_values[null_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULL values with the mean of the respective columns\n",
    "Data = Data.fillna(Data.mean())\n",
    "\n",
    "# Verify that there are no more NULL values\n",
    "null_values_after = Data.isnull().sum()\n",
    "print(\"Columns with NULL values after replacement:\")\n",
    "print(null_values_after[null_values_after > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "Data = Data.drop(columns=['Patient_ID'])\n",
    "numeric_columns = Data.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "Data[numeric_columns] = scaler.fit_transform(Data[numeric_columns])\n",
    "Data = pd.get_dummies(Data, drop_first=True)\n",
    "\n",
    "# Ensure all data is numeric and handle missing values\n",
    "Data = Data.apply(pd.to_numeric, errors='coerce')\n",
    "Data = Data.fillna(0)  # Fill missing values with 0 or use another strategy\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "bool_columns = Data.select_dtypes(include=['bool']).columns\n",
    "Data[bool_columns] = Data[bool_columns].astype(int)\n",
    "\n",
    "# Define target and features\n",
    "target = 'Survival_Prediction_Yes'\n",
    "features = Data.drop(columns=[target]).columns\n",
    "\n",
    "# Normalize the target variable\n",
    "Data[target] = Data[target].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = Data.corr()\n",
    "\n",
    "# Set a threshold for significant correlations\n",
    "threshold = 0.5\n",
    "\n",
    "# Filter the correlation matrix\n",
    "filtered_corr_matrix = correlation_matrix[(correlation_matrix >= threshold) | (correlation_matrix <= -threshold)]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(filtered_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, linecolor='black')\n",
    "plt.title('Filtered Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated features\n",
    "high_corr_pairs = [(col1, col2) for col1 in filtered_corr_matrix.columns for col2 in filtered_corr_matrix.columns if col1 != col2 and abs(filtered_corr_matrix.loc[col1, col2]) > threshold]\n",
    "\n",
    "# Print highly correlated pairs\n",
    "print(\"Highly correlated pairs:\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(pair, filtered_corr_matrix.loc[pair[0], pair[1]])\n",
    "\n",
    "# Remove one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for col1, col2 in high_corr_pairs:\n",
    "    # Example: Remove the second feature in each pair\n",
    "    features_to_remove.add(col2)\n",
    "\n",
    "# Drop the selected features from the dataset\n",
    "Data_reduced = Data.drop(columns=features_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "X = Data[features]\n",
    "y = Data[target]\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a DataFrame for permutation importances\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(perm_importance_df.sort_values(by='Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns where importance is below 0.1\n",
    "low_importance_features = perm_importance_df[perm_importance_df['Importance'] < 0.02]['Feature']\n",
    "Data = Data.drop(columns=low_importance_features)\n",
    "\n",
    "# Update features after dropping low-importance columns\n",
    "features = Data.drop(columns=[target]).columns\n",
    "\n",
    "# Show the updated list of features\n",
    "print(\"Updated list of features:\")\n",
    "print(Data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "X = Data[features]\n",
    "y = Data[target]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for train_index, test_index in kf.split(X_tensor):\n",
    "    X_train, X_test = X_tensor[train_index], X_tensor[test_index]\n",
    "    y_train, y_test = y_tensor[train_index], y_tensor[test_index]\n",
    "    \n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    input_size = X_train.shape[1]\n",
    "    model = SimpleNNClassifier(input_size)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(200):  # Number of epochs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test)\n",
    "            val_loss = criterion(val_outputs, y_test)\n",
    "        \n",
    "        if val_loss.item() < best_loss:\n",
    "            best_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        y_pred_class = (y_pred > 0.5).float()\n",
    "        accuracy = accuracy_score(y_test, y_pred_class)\n",
    "        f1 = f1_score(y_test, y_pred_class)\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Mean Accuracy: {np.mean(accuracy_scores):.4f}')\n",
    "print(f'Mean F1 Score: {np.mean(f1_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'ColorectalCancerPreditionModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNNClassifier(\n",
       "  (fc1): Linear(in_features=5, out_features=128, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "input_size = X_tensor.shape[1]\n",
    "loaded_model = SimpleNNClassifier(input_size)\n",
    "loaded_model.load_state_dict(torch.load('ColorectalCancerPreditionModel.pth'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1.0\n",
      "Predicted Class: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example new case (replace with actual feature values)\n",
    "new_case = np.array([[0.5, -1.2, 0.3, 1.5, -0.7]])  # Example feature values\n",
    "\n",
    "# Convert the new case to a PyTorch tensor\n",
    "new_case_tensor = torch.tensor(new_case, dtype=torch.float32)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    prediction = loaded_model(new_case_tensor)\n",
    "    prediction_class = (prediction > 0.5).float()\n",
    "\n",
    "print(f'Prediction: {prediction.item()}')\n",
    "print(f'Predicted Class: {prediction_class.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
